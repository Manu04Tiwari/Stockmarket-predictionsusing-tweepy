{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d215a44",
   "metadata": {},
   "source": [
    "Bert Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93853a65",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7371c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch\n",
    "#streamlit python 3.9.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import preprocess\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4330584",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1762cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in labeled stock tweet sentiment data\n",
    "data = pd.read_csv('train/stock_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43ea5c90",
   "metadata": {},
   "source": [
    "Process tweets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a16d68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the Tweets for NLP\n",
    "data = preprocess.Preprocess_Tweets(data)\n",
    "display(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74ddf228",
   "metadata": {},
   "source": [
    "Split the data in Training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training and test data into 80/20 split\n",
    "train_pct = .8\n",
    "np.random.seed(1)\n",
    "idx = np.random.permutation(len(data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e44dc5a",
   "metadata": {},
   "source": [
    "Model Training and Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93269148",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data['Text_Cleaned'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train = data['Sentiment'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train[y_train==-1] = 0\n",
    "\n",
    "\n",
    "X_test = data['Text_Cleaned'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test = data['Sentiment'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test[y_test==-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385caeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape )\n",
    "print(X_test.shape, y_test.shape )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bc5dcbe",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Bert NLP model tokenizer to encode tweets\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7895f873",
   "metadata": {},
   "source": [
    "Encoding Tweets for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a3a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the tweets for Bert model\n",
    "def preprocessing_for_bert(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    # For each tweet\n",
    "    for line in data:\n",
    "        # encode the data. Return input encoding and attention mask\n",
    "        encoding = tokenizer.encode_plus(\n",
    "                text=line, # data to process\n",
    "                add_special_tokens=True, # adds special chars [CLS] and [SEP] to encoding \n",
    "                padding='max_length', # pad the tweets with 0s to fit max length\n",
    "                max_length = MAX_LEN, # assign max length\n",
    "                truncation=True, # truncate tweets longer than max length\n",
    "                return_tensors=\"pt\", # return tensor as pytorch tensor\n",
    "                return_attention_mask=True # return the attention mask\n",
    "                )\n",
    "        \n",
    "    # add the encodings to the list\n",
    "        input_ids.append(encoding.get('input_ids'))\n",
    "        attention_masks.append(encoding.get('attention_mask'))\n",
    "        \n",
    "    # return the lists as tensors\n",
    "    input_ids = torch.concat(input_ids)\n",
    "    attention_masks = torch.concat(attention_masks)\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa9d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to determine max length for encoding\n",
    "encoded = [tokenizer.encode(sent, add_special_tokens=True) for sent in data['Text_Cleaned'].values]\n",
    "MAX_LEN = max([len(sent) for sent in encoded])\n",
    "print('Max length: ', MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55105c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode the tweets for Bert model\n",
    "def preprocessing_for_bert(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    # For each tweet\n",
    "    for line in data:\n",
    "        # encode the data. Return input encoding and attention mask\n",
    "        encoding = tokenizer.encode_plus(\n",
    "                text=line, # data to process\n",
    "                add_special_tokens=True, # adds special chars [CLS] and [SEP] to encoding \n",
    "                padding='max_length', # pad the tweets with 0s to fit max length\n",
    "                max_length = MAX_LEN, # assign max length\n",
    "                truncation=True, # truncate tweets longer than max length\n",
    "                return_tensors=\"pt\", # return tensor as pytorch tensor\n",
    "                return_attention_mask=True # return the attention mask\n",
    "                )\n",
    "\n",
    "        # add the encodings to the list\n",
    "        input_ids.append(encoding.get('input_ids'))\n",
    "        attention_masks.append(encoding.get('attention_mask'))\n",
    "    \n",
    "    # return the lists as tensors\n",
    "    input_ids = torch.concat(input_ids)\n",
    "    attention_masks = torch.concat(attention_masks)\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "\n",
    "# Use this to determine max length for encoding\n",
    "encoded = [tokenizer.encode(sent, add_special_tokens=True) for sent in data['Text_Cleaned'].values]\n",
    "MAX_LEN = max([len(sent) for sent in encoded])\n",
    "print('Max length: ', MAX_LEN)\n",
    "\n",
    "\n",
    "# Encode the train and test data for Bert\n",
    "X_train_inputs, X_train_masks = preprocessing_for_bert(X_train)\n",
    "X_test_inputs, X_test_masks = preprocessing_for_bert(X_test)\n",
    "\n",
    "# Get the train and test labels\n",
    "y_train_labels = torch.tensor(y_train)\n",
    "y_test_labels = torch.tensor(y_test)\n",
    "\n",
    "print(X_train_inputs.shape, X_train_masks.shape, y_train_labels.shape)\n",
    "print(X_test_inputs.shape, X_test_masks.shape, y_test_labels.shape)\n",
    "\n",
    "# Set batch size to 16. recommended 16 or 32 depending on GPU size \n",
    "batch_size = 16\n",
    "\n",
    "# Randomize the train data and define dataloader for model training \n",
    "train_data = TensorDataset(X_train_inputs, X_train_masks, y_train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Randomize the test data and define dataloader for model testing\n",
    "test_data = TensorDataset(X_test_inputs, X_test_masks, y_test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ee2ed2a",
   "metadata": {},
   "source": [
    "Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6e3eca-556a-4ee7-b48e-ea5b9f1bc998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Bert NLP Classifier\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, freeze=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        # Define the neurons for the final layer\n",
    "        input_layer = 768\n",
    "        hidden_layer = 50\n",
    "        output_layer = 2\n",
    "\n",
    "        # Use the pretrained Bert model for first section of NN\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Define a final layer to attach to the Bert model for custom classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_layer, hidden_layer), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_layer, output_layer))\n",
    "\n",
    "        # Freeze the model from updating\n",
    "        if freeze:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    # Return classification from Bert model \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask)\n",
    "        h_cls = outputs[0][:, 0, :]\n",
    "        logits = self.classifier(h_cls)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "# Set random seed for repeatability\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Check if GPU is available and assign device \n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "# Initialize Bert Classifier\n",
    "model = BertClassifier(freeze=False)\n",
    "\n",
    "# Send model to device (GPU if available)\n",
    "model.to(device)\n",
    "\n",
    "# Define model hyperparameters\n",
    "epochs = 4\n",
    "steps = len(train_dataloader) * epochs\n",
    "learning_rate = 5e-5\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Define Adam optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n",
    "\n",
    "# Define scheduler for training the optimizer \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n",
    "\n",
    "# Define cross entropy loss function \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# For the number of epochs\n",
    "for e in range(epochs):\n",
    "    # Assign model to train\n",
    "    model.train()\n",
    "\n",
    "    # Intialize loss to zero\n",
    "    train_loss = 0\n",
    "    \n",
    "    # For each batch\n",
    "    for batch in train_dataloader:\n",
    "        # Get batch inputs, masks and labels \n",
    "        batch_inputs, batch_masks, batch_labels = batch\n",
    "        \n",
    "        # Send variables to device (GPU if available)\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_masks = batch_masks.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # Reset the model gradient\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Get classification of encoded values\n",
    "        logits = model(batch_inputs, batch_masks)\n",
    "        \n",
    "        # Calculate loss based on predictions and known values\n",
    "        loss = loss_function(logits, batch_labels)\n",
    "        \n",
    "        # Add loss to the running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Update the model weights based on the loss \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over batch\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # Assign the model to evaluate    \n",
    "    model.eval()\n",
    "\n",
    "    # Initialize losses\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    # For each batch\n",
    "    for batch in test_dataloader:\n",
    "        # Get encoding inputs, masks and labels\n",
    "        batch_inputs, batch_masks, batch_labels = batch\n",
    "        \n",
    "        # Send variables to device (GPU if available)\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_masks = batch_masks.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # Predict the input values without updating the model \n",
    "        with torch.no_grad():\n",
    "            logits = model(batch_inputs, batch_masks)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_function(logits, batch_labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Convert predictions to 0 and 1\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate accuracy of model on test data \n",
    "        accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n",
    "        test_acc += accuracy\n",
    "\n",
    "    # Calculate average loss and accuracy per each batch\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "    # Print epoch information \n",
    "    print('Epoch: %d  |  Train Loss: %1.5f  |  Test Loss: %1.5f  |  Test Accuracy: %1.2f'%(e+1, train_loss, test_loss, test_acc))\n",
    "    \n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'stock_sentiment_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dcfe2b-967c-4969-a086-fb326815bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "# Get the list of stock data to convert\n",
    "files = os.listdir('data/')\n",
    "\n",
    "# for each stock files\n",
    "for x in range(len(files)):\n",
    "    # open the excel file on the Stream sheet\n",
    "    stock = pd.read_excel('data/'+files[x] + '/export_dashboard_' + files[x], sheet_name='Stream')\n",
    "\n",
    "    # Assign the ticker name as a column\n",
    "    stock['Ticker'] = files[x].split('_')[0]\n",
    "    \n",
    "    # Convert string date times to datetime\n",
    "    stock['Date'] = pd.to_datetime(stock['Date'])\n",
    "    stock['Hour'] = stock['Hour'].apply(lambda t: pd.Timedelta(hours=int(t[:2]), minutes=int(t[3:])))\n",
    "    stock['Datetime'] = stock['Date'] + stock['Hour']\n",
    "\n",
    "    # Rename column that holds the tweets content\n",
    "    stock.rename(columns = {'Tweet content':'Text'}, inplace = True)\n",
    "\n",
    "    # Pre process the tweet content\n",
    "    stock = preprocess.Preprocess_Tweets(stock)\n",
    "\n",
    "    # Remove excess columns\n",
    "    stock = stock[['Tweet Id', 'Ticker', 'Datetime', 'Text', 'Text_Cleaned', 'Favs', 'RTs', 'Followers', 'Following', 'Is a RT']]\n",
    "    \n",
    "    # Fill NAs in Favs, RTs, Followers and Following with 0\n",
    "    stock = stock.fillna(0)\n",
    "\n",
    "    # Encode processed tweets for Bert NLP model\n",
    "    stock_inputs, stock_masks = preprocessing_for_bert(stock['Text_Cleaned'].values)\n",
    "\n",
    "    # Put stock data in PyTorch dataloader for processing \n",
    "    stock_data = TensorDataset(stock_inputs, stock_masks)\n",
    "    stock_sampler = RandomSampler(stock_data)\n",
    "    stock_dataloader = DataLoader(stock_data, sampler=stock_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Assign model to evaluate \n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # For each batch\n",
    "    for batch in stock_dataloader:\n",
    "        # Get encoded inputs and masks \n",
    "        batch_inputs, batch_masks = batch\n",
    "\n",
    "        # Send variables to device (GPU if available)\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_masks = batch_masks.to(device)\n",
    "\n",
    "        # Predict classes with Bert for given inputs \n",
    "        with torch.no_grad():\n",
    "            logits = model(batch_inputs, batch_masks)\n",
    "\n",
    "        # Convert predictions to 0s and 1s\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        predictions.append(preds)\n",
    "\n",
    "    # Combine all batch predictions\n",
    "    predictions = torch.cat(predictions).cpu().numpy()\n",
    "    \n",
    "    # Add predictions to stock dataframe\n",
    "    stock['Sentiment'] = predictions\n",
    "    \n",
    "    # save predictions as new csv\n",
    "    stock.to_csv('data/'+files[x] +'/stock_data_sentiment.csv', index=False)\n",
    "    \n",
    "    # Show stock names as they are completed \n",
    "    print(files[x].split('_')[0], '- completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea57530-2bac-4033-9f83-dcdc3207dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch_inputs, batch_masks, batch_labels = batch\n",
    "\n",
    "    batch_inputs = batch_inputs.to(device)\n",
    "    batch_masks = batch_masks.to(device)\n",
    "    batch_labels = batch_labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(batch_inputs, batch_masks)\n",
    "\n",
    "\n",
    "    preds = torch.argmax(logits, dim=1).flatten()\n",
    "    predictions.append(preds)\n",
    "        \n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "\n",
    "negatives = np.where(y_test==0)[0]\n",
    "TNs = np.where( (y_test==0) & (y_test==predictions) )[0]\n",
    "print(len(TNs)/len(negatives))\n",
    "\n",
    "positives = np.where(y_test==1)[0]\n",
    "TPs = np.where( (y_test==1) & (y_test==predictions) )[0]\n",
    "print(len(TPs)/len(positives))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
